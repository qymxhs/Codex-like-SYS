
1.  **Data Collection:** Gathering user code and feedback.
2.  **Fine-tuning:** Using this data to fine-tune OpenAI (specifically mentioning "o3" which I'll interpret as GPT-3 series like `gpt-3.5-turbo` or newer models that support fine-tuning).
    *   Interacting with a "GitHub MCP Server" (this implies an existing or to-be-built service that facilitates this iterative process, possibly by managing critique steps or orchestrating LLM calls).
    *   The agent itself will act as a "code assistant tool" capable of editing files and running commands as part of this MCP loop.

This is a sophisticated approach that aims to create a highly specialized and self-improving AI coding assistant.

Let's break down how we can architect and implement key parts of this, focusing on the **Agent's role** and its interaction with a hypothetical **MCP Server** and the **fine-tuning data collection**.

---

## ðŸš€ Architecture for MCP and Fine-tuning Data Collection ðŸš€

```mermaid
graph TD
    User --> PlatformUI[Codex-like Platform UI]

    subgraph "Codex Platform Backend (Go - API & Orchestration)"
        APIService[API Service]
        TaskQueue[Task Queue (Kafka/Redis)]
        DB[Database (MySQL - Tasks, Feedback)]
        FineTuneDataStore[Fine-tuning Data Store (S3/COS/DB)]
    end

    subgraph "MCP Orchestration (Hypothetical 'GitHub MCP Server' or similar component)"
        MCPServer[MCP Server/Orchestrator]
    end

    subgraph "Agent Execution Environment (Kubernetes Pod)"
        AgentCode[Python Agent (codex-agent)]
        UserCodeVolume[/app/code - User's Code]
        OutputVolume[/app/output - Logs, Diffs]
    end

    subgraph "External Services"
        OpenAI_FineTuned[OpenAI API (Fine-tuned Model)]
        OpenAI_Base[OpenAI API (Base Model for critique/initial pass)]
        GitHubAPI[GitHub API (for PRs, etc.)]
    end

    PlatformUI -- Task Submission --> APIService
    APIService -- Task Details --> DB
    APIService -- User Code/Feedback for Fine-tuning --> FineTuneDataStore
    APIService -- Enqueue Task --> TaskQueue

    TaskQueue -- Task Message --> AgentCode

    AgentCode -- Initial Request / Iteration --> MCPServer
    MCPServer -- LLM Call (Initial/Refine) --> OpenAI_FineTuned
    MCPServer -- LLM Call (Critique) --> OpenAI_Base
    MCPServer -- Instructions (Edit File, Run Command) --> AgentCode

    AgentCode -- Edit File --> UserCodeVolume
    AgentCode -- Run Command (Linter, Tests) --> UserCodeVolume
    AgentCode -- Tool Results / Code State --> MCPServer

    AgentCode -- Final Diff/PR --> OutputVolume
    AgentCode -- Log User Actions/Feedback for Fine-tuning --> FineTuneDataStore
    AgentCode -- Create PR --> GitHubAPI

    FineTuneDataStore -- Data for --> TrainingPipeline[Fine-tuning Pipeline]
    TrainingPipeline -- Updates --> OpenAI_FineTuned

    %% User Feedback Loop
    PlatformUI -- User Feedback on Agent Output --> APIService
```

**Key Components & Flow:**

1.  **Platform UI & Backend:**
    *   Users submit tasks (e.g., "Refactor this function," "Fix this bug described in issue #123").
    *   The backend stores task details and, crucially, **collects user feedback** on the agent's performance. This feedback is vital for fine-tuning.
    *   It also stores snapshots of (code_before, user_instruction, agent_action_sequence, code_after, user_feedback_rating, critique) as **fine-tuning data**.

2.  **MCP Server/Orchestrator:**
    *   This is a central piece. It could be a dedicated service you build or a more abstract concept managed by the agent's internal logic.
    *   **Responsibilities:**
        *   Receives the initial task from the Agent.
        *   Manages the iterative loop:
            1.  **Plan/Initial Action:** Calls the (potentially fine-tuned) OpenAI model to get an initial plan or code modification.
            2.  **Instruct Agent:** If the LLM suggests an action (e.g., "edit file X", "run `pylint` on Y"), it sends this instruction to the Agent.
            3.  **Receive Agent Result:** Gets the outcome of the Agent's action (e.g., diff, command output).
            4.  **Critique:** Calls an OpenAI model (base or specialized critique model) to evaluate the current state/action against the goal. The critique prompt would be key here.
            5.  **Refine:** Based on the critique, calls the OpenAI model again to generate a better plan or code.
        *   Decides when the loop should terminate (e.g., critique is positive, max iterations reached, user intervention).

3.  **Python Agent (Enhanced for MCP):**
    *   **Communicates with MCP Server:** Sends its current state, results of actions, and receives new instructions.
    *   **Tool Execution:** Has robust `edit_file`, `run_command` tools.
    *   **Fine-tuning Data Logging:** Logs its interactions and the LLM's reasoning/tool calls in a structured format suitable for fine-tuning. For example:
        ```json
        {
            "task_id": "xyz",
            "iteration": 1,
            "prompt_to_llm": "...",
            "llm_response": {"tool_call": {"name": "edit_file", "arguments": {"file_path": "...", "new_content": "..."}}},
            "action_taken": "edit_file",
            "action_parameters": {"file_path": "...", "new_content": "..."},
            "action_result_status": "success", // or "failure"
            "code_snapshot_before": "...", // optional, or diff
            "code_snapshot_after": "..."   // optional, or diff
        }
        ```
        This data, along with user feedback, forms the basis for fine-tuning.

4.  **Fine-tuning Pipeline:**
    *   A separate process (manual or automated) that takes data from `FineTuneDataStore`.
    *   Formats it into the required OpenAI fine-tuning format (e.g., JSONL with `{"messages": [{"role": "system", ...}, {"role": "user", ...}, {"role": "assistant", ...}]}`).
    *   Uses the OpenAI API to create and manage fine-tuning jobs.
    *   The `OPENAI_API_KEY` used by the agent would then point to this newly fine-tuned model.

---

## Implementing Agent Tools for MCP

Let's refine the Agent's tool execution to fit better into an MCP loop where an orchestrator (MCP Server or internal agent logic) decides which tool to call.

### `dockerfiles/agent/agent.py` (Focus on Tool Execution and MCP Interaction Logic)

```python
import openai
import os
import sys
import difflib
import logging
import subprocess
import json
from pathlib import Path
from dotenv import load_dotenv
# (RAG components can be kept if needed for initial context gathering before MCP loop)

# --- Configuration ---
# ... (as before, ensure OPENAI_API_KEY points to the potentially fine-tuned model)
MODEL_NAME_MAIN_AGENT = os.getenv("OPENAI_FINETUNED_MODEL_ID", "gpt-4o") # Use fine-tuned model
MODEL_NAME_CRITIQUE = os.getenv("OPENAI_CRITIQUE_MODEL_ID", "gpt-4o") # Or a different base model for critique
MCP_SERVER_URL = os.getenv("MCP_SERVER_URL") # URL for the MCP Orchestrator

MAX_MCP_ITERATIONS = 7
CODE_DIR = Path("/app/code")
OUTPUT_DIR = Path("/app/output") # For final outputs and detailed logs
FINE_TUNING_LOG_DIR = OUTPUT_DIR / "finetuning_data" # Store structured logs for fine-tuning
FINE_TUNING_LOG_DIR.mkdir(parents=True, exist_ok=True)

# --- Logging ---
# ... (standard logging setup as before) ...

# --- Tool Implementations ---
# These tools are now designed to be called explicitly by the agent's main loop
# based on instructions from an orchestrator (MCP Server or internal logic).

def tool_edit_file(file_path: str, new_content: str = None, diff_patch: str = None, insert_after_line: int = -1, replace_lines: tuple = None):
    """
    Edits a file. Can apply new_content, a diff_patch, insert text, or replace lines.
    file_path: Relative to /app/code.
    new_content: If provided, replaces the entire file content.
    diff_patch: If provided, applies a diff patch to the file.
    insert_after_line: If > -1, inserts new_content after this 0-indexed line.
    replace_lines: A tuple (start_line, end_line) for lines to be replaced by new_content (0-indexed, inclusive).
    Returns: {"status": "success/failure", "message": "...", "diff": "actual_diff_applied"}
    """
    logging.info(f"Tool: edit_file called for '{file_path}'")
    full_file_path = (CODE_DIR / file_path).resolve()
    if not str(full_file_path).startswith(str(CODE_DIR.resolve())) or ".." in file_path:
        return {"status": "failure", "message": "Security: Path outside allowed directory."}
    if not full_file_path.exists() and not (new_content is not None and diff_patch is None): # Allow creating new file only with full new_content
        return {"status": "failure", "message": f"File not found: {file_path}"}

    try:
        original_content = ""
        if full_file_path.exists():
            with open(full_file_path, "r", encoding="utf-8") as f:
                original_content = f.read()
        
        modified_content = original_content

        if new_content is not None and diff_patch is None: # Prioritize full content replacement
            modified_content = new_content
        elif diff_patch is not None:
            # Apply diff_patch (this is complex; using 'patch' utility or a Python library is better)
            # Simplified placeholder:
            logging.warning("Applying diff_patch is non-trivial. This is a placeholder.")
            # For a real implementation, use `subprocess.run(['patch', str(full_file_path)], input=diff_patch, text=True)`
            # or a Python diff/patch library.
            # For now, let's assume the LLM might provide new_content *after* calculating a diff.
            # If the LLM is good, it might just give the new full content.
            # For now, if diff_patch is given, we assume new_content IS the patched content.
            if new_content is not None:
                 modified_content = new_content
            else:
                return {"status": "failure", "message": "Diff patch application requires new_content to be the patched version for this simplified tool."}

        elif insert_after_line > -1 and new_content is not None:
            lines = original_content.splitlines(True)
            if insert_after_line < len(lines):
                lines.insert(insert_after_line + 1, new_content + ('\n' if not new_content.endswith('\n') else ''))
                modified_content = "".join(lines)
            else: # Append if line number is too high
                modified_content = original_content + ('\n' if original_content and not original_content.endswith('\n') else '') + new_content
        elif replace_lines and new_content is not None:
            start, end = replace_lines
            lines = original_content.splitlines(True)
            if 0 <= start < len(lines) and start <= end:
                end = min(end, len(lines)-1)
                # Ensure new_content has trailing newline if replacing multiple lines or last line
                nc_with_nl = new_content + ('\n' if (end > start or (end == len(lines)-1)) and not new_content.endswith('\n') else '')
                modified_content = "".join(lines[:start]) + nc_with_nl + "".join(lines[end+1:])
            else:
                 return {"status": "failure", "message": f"Invalid line numbers for replacement: {replace_lines}"}


        with open(full_file_path, "w", encoding="utf-8") as f:
            f.write(modified_content)

        # Generate diff of what actually changed
        actual_diff_list = list(difflib.unified_diff(
            original_content.splitlines(keepends=True),
            modified_content.splitlines(keepends=True),
            fromfile=f"a/{file_path}", tofile=f"b/{file_path}", lineterm=""
        ))
        actual_diff_str = "".join(actual_diff_list)

        return {"status": "success", "message": f"File '{file_path}' edited.", "diff": actual_diff_str}
    except Exception as e:
        logging.error(f"Tool: edit_file error for '{file_path}': {e}")
        return {"status": "failure", "message": str(e)}


def tool_run_command(command_string: str, target_directory: str = "."):
    """
    Executes a shell command.
    target_directory: Relative to /app/code.
    Returns: {"status": "success/failure", "stdout": "...", "stderr": "...", "exit_code": ...}
    """
    logging.info(f"Tool: run_command: '{command_string}' in '{target_directory}'")
    full_target_dir = (CODE_DIR / target_directory).resolve()
    if not str(full_target_dir).startswith(str(CODE_DIR.resolve())) or ".." in target_directory :
        return {"status": "failure", "stdout": "", "stderr": "Security: Path outside allowed directory.", "exit_code": -1}
    if not full_target_dir.is_dir():
        return {"status": "failure", "stdout": "", "stderr": f"Directory not found: {target_directory}", "exit_code": -1}

    try:
        # Using shell=True is risky. Consider parsing command_string into cmd and args array.
        process = subprocess.run(command_string, cwd=str(full_target_dir), shell=True, capture_output=True, text=True, timeout=60)
        stdout_brief = (process.stdout or "").strip()[:2000] # Limit output
        stderr_brief = (process.stderr or "").strip()[:2000] # Limit output
        return {
            "status": "success" if process.returncode == 0 else "failure",
            "stdout": stdout_brief,
            "stderr": stderr_brief,
            "exit_code": process.returncode
        }
    except subprocess.TimeoutExpired:
        return {"status": "failure", "stdout": "", "stderr": "Command timed out after 60 seconds.", "exit_code": -2}
    except Exception as e:
        logging.error(f"Tool: run_command error: {e}")
        return {"status": "failure", "stdout": "", "stderr": str(e), "exit_code": -1}

def tool_read_file_content(file_path: str):
    """Reads content of a file."""
    # ... (implementation from previous version, ensuring security checks) ...
    logging.info(f"Tool: read_file_content for '{file_path}'")
    full_file_path = (CODE_DIR / file_path).resolve()
    if not str(full_file_path).startswith(str(CODE_DIR.resolve())) or ".." in file_path:
        return {"status": "failure", "content": None, "message": "Security: Path outside allowed directory."}
    if not full_file_path.is_file():
        return {"status": "failure", "content": None, "message": f"File not found: {file_path}"}
    try:
        with open(full_file_path, "r", encoding="utf-8") as f:
            content = f.read()
        return {"status": "success", "content": content[:5000], "message": "Content retrieved (may be truncated)."} # Limit for context
    except Exception as e:
        return {"status": "failure", "content": None, "message": str(e)}

# --- MCP Interaction and Main Loop ---

def log_for_finetuning(task_id, iteration, role, content, tool_name=None, tool_args=None, tool_result=None):
    """Logs interaction in a structured way for fine-tuning."""
    log_entry = {
        "timestamp": time.time(),
        "task_id": task_id,
        "iteration": iteration,
        "role": role, # "user_instruction", "llm_thought", "llm_tool_call", "tool_execution", "critique"
        "content": content,
    }
    if tool_name:
        log_entry["tool_name"] = tool_name
    if tool_args:
        log_entry["tool_args"] = tool_args
    if tool_result: # This would be a summary or key parts of the result
        log_entry["tool_result_summary"] = str(tool_result)[:500]

    with open(FINE_TUNING_LOG_DIR / f"{task_id}_finetune_log.jsonl", "a") as f:
        f.write(json.dumps(log_entry) + "\n")


def call_mcp_server(task_id, iteration, current_code_state_summary, previous_actions_and_results):
    """Simulates calling an external MCP Server.
    In a real scenario, this would be an HTTP request.
    For this example, we'll have it call an OpenAI model directly for planning/critique.
    """
    # Construct messages for the MCP orchestrator LLM
    messages = [
        {"role": "system", "content": f"You are an MCP orchestrator for a coding task (ID: {task_id}). Iteration: {iteration}. Your goal is to guide an agent to complete a user's request. You can instruct the agent to 'edit_file', 'run_command', or 'read_file_content'. After the agent acts, you will receive the result. Then, critique the progress and decide the next action. If the task is complete, respond with 'TASK_COMPLETE: <final message or code>' or 'TASK_COMPLETE_WITH_CODE: ```<code>```'."},
        {"role": "user", "content": f"Initial User Task: {initial_task_description_global}\nTarget File: {target_file_rel_path_global}\n\nCurrent Code State Summary:\n{current_code_state_summary}\n\nPrevious Actions & Results:\n{json.dumps(previous_actions_and_results, indent=2)}"},
        {"role": "user", "content": "Based on the above, what is the next best action for the agent (edit_file, run_command, read_file_content with specific parameters), or is the task complete? If calling a tool, respond ONLY with a JSON object like: {'tool_to_call': 'tool_name', 'arguments': {'arg1': 'val1', ...}}. If critiquing and planning next step, explain your reasoning then suggest the tool call JSON. If task is complete, state TASK_COMPLETE or TASK_COMPLETE_WITH_CODE."}
    ]
    
    log_for_finetuning(task_id, iteration, "mcp_orchestrator_prompt", messages)

    client = openai.OpenAI(api_key=OPENAI_API_KEY)
    response = client.chat.completions.create(
        model=MODEL_NAME_CRITIQUE, # Use critique model for orchestration
        messages=messages,
        temperature=0.3 # More deterministic for planning
    )
    mcp_decision = response.choices[0].message.content
    log_for_finetuning(task_id, iteration, "mcp_orchestrator_response", mcp_decision)
    logging.info(f"MCP Orchestrator Decision (Iter {iteration}):\n{mcp_decision}")
    return mcp_decision


# Global vars to hold initial task details, set in main()
initial_task_description_global = ""
target_file_rel_path_global = ""
original_code_global = ""


def main_mcp_loop(task_id, task_description, target_file_rel, original_code):
    global initial_task_description_global, target_file_rel_path_global, original_code_global
    initial_task_description_global = task_description
    target_file_rel_path_global = target_file_rel
    original_code_global = original_code

    current_code_content = original_code
    actions_history = [] # List of {"action": ..., "result": ...}

    for i in range(MAX_MCP_ITERATIONS):
        logging.info(f"--- MCP Iteration {i+1} ---")

        # Summarize current state for MCP server/orchestrator
        # This could be more sophisticated (e.g., diff from original)
        code_state_summary = f"File '{target_file_rel}' current content (first 500 chars):\n{current_code_content[:500]}"
        if len(current_code_content) > 500: code_state_summary += "\n...(truncated)"

        # Call MCP Server (or internal orchestrator logic) for next action/critique
        mcp_response_str = call_mcp_server(task_id, i + 1, code_state_summary, actions_history)

        tool_call_json = None
        try:
            # Attempt to parse if LLM directly gave JSON for tool call
            # A more robust parser would look for JSON within the mcp_response_str
            if "{" in mcp_response_str and "}" in mcp_response_str:
                potential_json_part = mcp_response_str[mcp_response_str.find("{") : mcp_response_str.rfind("}")+1]
                parsed_decision = json.loads(potential_json_part)
                if "tool_to_call" in parsed_decision and "arguments" in parsed_decision:
                    tool_call_json = parsed_decision
        except json.JSONDecodeError:
            logging.info("MCP response was not a direct JSON tool call. Treating as natural language.")
            # If not a direct JSON, LLM might be explaining or saying task is complete.

        action_result = None
        if tool_call_json:
            tool_name = tool_call_json.get("tool_to_call")
            tool_args = tool_call_json.get("arguments", {})
            logging.info(f"MCP instructs to call tool: {tool_name} with args: {tool_args}")
            log_for_finetuning(task_id, i+1, "agent_tool_execution_start", f"Calling {tool_name}", tool_name, tool_args)

            if tool_name == "edit_file":
                action_result = tool_edit_file(**tool_args)
                if action_result.get("status") == "success":
                    # Update current_code_content if edit was successful
                    # This requires tool_edit_file to be robust or re-read the file
                    with open(CODE_DIR / tool_args.get("file_path"), "r", encoding="utf-8") as f:
                        current_code_content = f.read()
            elif tool_name == "run_command":
                action_result = tool_run_command(**tool_args)
            elif tool_name == "read_file_content":
                action_result = tool_read_file_content(**tool_args)
            else:
                action_result = {"status": "failure", "message": f"Unknown tool: {tool_name}"}
            
            actions_history.append({"action": tool_call_json, "result": action_result})
            log_for_finetuning(task_id, i+1, "agent_tool_execution_result", action_result.get("message", str(action_result)), tool_name, tool_args, action_result)

        elif "TASK_COMPLETE_WITH_CODE:" in mcp_response_str.upper():
            logging.info("MCP signals task complete with code.")
            # Extract code from ```code``` block
            code_match = re.search(r"```(?:[a-zA-Z0-9_.-]+\n)?(.*?)```", mcp_response_str, re.DOTALL)
            if code_match:
                final_code = code_match.group(1).strip()
                logging.info(f"Final code extracted:\n{final_code}")
                # This final code should ideally be for the target_file_rel_path
                # We can write it and then generate the diff against original_code_global
                with open(CODE_DIR / target_file_rel, "w", encoding="utf-8") as f:
                    f.write(final_code)
                current_code_content = final_code # update for final diff
            else:
                logging.warning("TASK_COMPLETE_WITH_CODE but no code block found. Using current code state.")
            break # Exit MCP loop
        elif "TASK_COMPLETE:" in mcp_response_str.upper():
            logging.info("MCP signals task complete (no specific code output).")
            final_message = mcp_response_str.split("TASK_COMPLETE:", 1)[-1].strip()
            (OUTPUT_DIR / "llm_answer.txt").write_text(final_message, encoding='utf-8')
            break # Exit MCP loop
        else:
            logging.info("MCP provided reasoning or non-tool instruction. Loop continues for critique/next plan.")
            # The actions_history will include this mcp_response_str for the next iteration's context

        if len(actions_history) > 10 : # Prune history to keep context manageable
            actions_history = actions_history[-10:]


    # --- After MCP Loop ---
    logging.info("MCP loop finished.")
    final_diff = "".join(difflib.unified_diff(
        original_code_global.splitlines(keepends=True),
        current_code_content.splitlines(keepends=True), # current_code_content has the final state
        fromfile=f"a/{target_file_rel}", tofile=f"b/{target_file_rel}", lineterm=""
    ))
    
    (OUTPUT_DIR / "final_code.txt").write_text(current_code_content, encoding='utf-8')
    (OUTPUT_DIR / "diff.patch").write_text(final_diff, encoding='utf-8')
    logging.info(f"Final diff patch saved to {OUTPUT_DIR / 'diff.patch'}")

    # Log final state for fine-tuning (if user provides feedback later)
    log_for_finetuning(task_id, "final", "final_code_state", {"file": target_file_rel, "content": current_code_content[:1000]})

    # PR creation would use current_code_content (which is now written to the file)
    # ... (GitHub PR creation logic if applicable, using the file at CODE_DIR / target_file_rel) ...


def main():
    logging.info("Agent script started (MCP & Fine-tuning Log Enabled).")
    if len(sys.argv) < 4: # task_id, task_description, target_file, is_github_repo
        logging.error("Usage: python agent.py <task_id> <task_description> <target_file_relative_path> <is_github_repo_str>")
        sys.exit(1)

    task_id_arg = sys.argv[1]
    task_description_arg = sys.argv[2]
    target_file_rel_path_arg = sys.argv[3]
    # is_github_repo_arg = sys.argv[4].lower() == 'true' # if needed later for PR

    logging.info(f"Task ID: {task_id_arg}")
    logging.info(f"Task Description: {task_description_arg}")
    logging.info(f"Target File: {target_file_rel_path_arg}")

    target_file_abs_path = CODE_DIR / target_file_rel_path_arg
    if not target_file_abs_path.is_file():
        logging.error(f"Target file not found: {target_file_abs_path}")
        # Log for fine-tuning that initial setup failed
        log_for_finetuning(task_id_arg, 0, "error_initial_setup", {"message": f"Target file not found: {target_file_abs_path}"})
        sys.exit(1)
    
    # Log initial user instruction for fine-tuning
    log_for_finetuning(task_id_arg, 0, "user_initial_instruction", {"description": task_description_arg, "target_file": target_file_rel_path_arg})


    with open(target_file_abs_path, "r", encoding='utf-8') as f:
        original_code_content = f.read()

    # Run setup.sh if it exists (as before)
    # ...

    # Start the MCP loop
    main_mcp_loop(task_id_arg, task_description_arg, target_file_rel_path_arg, original_code_content)

    # GitHub PR creation if applicable (use is_github_repo_arg)
    # if is_github_repo_arg and GITHUB_TOKEN:
    #    create_github_pr(CODE_DIR, target_file_rel_path_arg, task_description_arg)


    logging.info(f"Agent script for task {task_id_arg} finished.")


if __name__ == "__main__":
    # For direct invocation, simulate args
    if len(sys.argv) == 1:
        print("Simulating args for local test run...")
        # Create dummy files for testing
        (CODE_DIR / "test_code.py").write_text("def hello():\n    print('world')\n")
        sys.argv.extend([
            "test_task_001",                     # task_id
            "Add a comment explaining the hello function and rename it to greet_world.", # task_description
            "test_code.py",                      # target_file
            "false"                              # is_github_repo
        ])

    main()
```

### Key Changes and Considerations:

1.  **`MCP_SERVER_URL`:** This environment variable would point to your MCP orchestrator. The `call_mcp_server` function currently *simulates* this by directly calling OpenAI for orchestration logic. In a real system, it would make an HTTP request.
2.  **Tool Re-design:**
    *   Tools like `tool_edit_file` are now more versatile (can take full content, diff, or line-based edits).
    *   They return structured JSON (`{"status": ..., "message": ..., "data": ...}`).
3.  **`log_for_finetuning`:** This function is crucial. It logs key information from each step of the MCP process in a JSONL format. This data includes:
    *   User's initial request.
    *   Prompts sent to the orchestrator/LLM.
    *   LLM's thoughts/decisions.
    *   Tool calls (name, arguments).
    *   Tool execution results.
    *   (Later, you'd add user feedback collected via the platform UI to these logs).
4.  **`main_mcp_loop`:**
    *   This is the core iterative loop.
    *   It calls `call_mcp_server` (or the internal orchestrator logic).
    *   Parses the orchestrator's response to see if a tool needs to be called or if the task is complete.
    *   Executes the tool and updates the `actions_history`.
    *   The loop continues until the task is marked complete or max iterations are hit.
5.  **Orchestrator Logic (`call_mcp_server` simulation):**
    *   The prompt to the "orchestrator LLM" (here, `MODEL_NAME_CRITIQUE`) is designed to make it reason about the current state, previous actions, and decide the next best step (call a tool or declare completion).
    *   It's instructed to output JSON for tool calls, which the agent tries to parse.
6.  **Fine-tuned Model Usage:**
    *   `MODEL_NAME_MAIN_AGENT` would eventually point to your fine-tuned model ID. This model would be used for tasks like generating code modifications or initial plans.
    *   `MODEL_NAME_CRITIQUE` could be a powerful base model (like GPT-4) or another fine-tuned model specialized in critiquing code or plans.
7.  **Task ID Propagation:** The `task_id` is now passed as an argument to the agent, essential for correlating fine-tuning logs. The backend (API handler creating the Docker/K8s job) needs to pass this.

### Backend Changes (Conceptual):

*   **API Handler:** When creating the agent job, pass the `task_id` as a command-line argument to `agent.py`.
*   **Fine-tuning Data Ingestion:**
    *   The `FINE_TUNING_LOG_DIR` (e.g., `/app/output/finetuning_data`) within the agent container needs to be collected after the agent finishes.
    *   The Worker service (Go) would be responsible for uploading files from this directory (e.g., `task_id_finetune_log.jsonl`) to the `FineTuneDataStore` (S3/COS).
*   **User Feedback Endpoint:** A new API endpoint where the frontend can submit user feedback (e.g., rating, corrections, alternative solutions) for a given `task_id`. This feedback would also be stored in `FineTuneDataStore` and correlated with the agent's logs.

### Next Steps:

1.  **Build the "MCP Server" Logic:** Decide if this will be a separate microservice or if the agent's `call_mcp_server` will evolve into a more complex internal state machine that orchestrates the LLM calls for planning, action, and critique.
2.  **Refine Prompts for Orchestrator/Critique LLM:** This is critical. The quality of the MCP loop heavily depends on how well the orchestrator LLM can reason, critique, and plan.
3.  **Implement the Fine-tuning Pipeline:** Set up scripts/processes to regularly collect data from `FineTuneDataStore`, format it, and run OpenAI fine-tuning jobs.
4.  **Robust Tooling:** Make `tool_edit_file` (especially diff application) and `tool_run_command` more robust and secure.
5.  **User Feedback Integration:** Ensure the platform UI allows users to give detailed feedback, and this feedback is linked to the agent's actions for that task.

This is a very ambitious and exciting direction, leading towards a truly intelligent and adaptive AI coding assistant!
